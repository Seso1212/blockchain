{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f79d99ef",
      "metadata": {
        "id": "f79d99ef"
      },
      "source": [
        "# Train your first üê∏ TTS model üí´\n",
        "\n",
        "### üëã Hello and welcome to Coqui (üê∏) TTS\n",
        "\n",
        "The goal of this notebook is to show you a **typical workflow** for **training** and **testing** a TTS model with üê∏.\n",
        "\n",
        "Let's train a very small model on a very small amount of data so we can iterate quickly.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Download data and format it for üê∏ TTS.\n",
        "2. Configure the training and testing runs.\n",
        "3. Train a new model.\n",
        "4. Test the model and display its performance.\n",
        "\n",
        "So, let's jump right in!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2aec78",
      "metadata": {
        "id": "fa2aec78"
      },
      "outputs": [],
      "source": [
        "## Install Coqui TTS\n",
        "! pip install -U pip\n",
        "! pip install TTS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5fe49c",
      "metadata": {
        "id": "be5fe49c"
      },
      "source": [
        "## ‚úÖ Data Preparation\n",
        "\n",
        "### **First things first**: we need some data.\n",
        "\n",
        "We're training a Text-to-Speech model, so we need some _text_ and we need some _speech_. Specificially, we want _transcribed speech_. The speech must be divided into audio clips and each clip needs transcription. More details about data requirements such as recording characteristics, background noise and vocabulary coverage can be found in the [üê∏TTS documentation](https://tts.readthedocs.io/en/latest/formatting_your_dataset.html).\n",
        "\n",
        "If you have a single audio file and you need to **split** it into clips. It is also important to use a lossless audio file format to prevent compression artifacts. We recommend using **wav** file format.\n",
        "\n",
        "The data format we will be adopting for this tutorial is taken from the widely-used  **LJSpeech** dataset, where **waves** are collected under a folder:\n",
        "\n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "/wavs<br />\n",
        " &emsp;| - audio1.wav<br />\n",
        " &emsp;| - audio2.wav<br />\n",
        " &emsp;| - audio3.wav<br />\n",
        "  ...<br />\n",
        "</span>\n",
        "\n",
        "and a **metadata.csv** file will have the audio file name in parallel to the transcript, delimited by `|`:\n",
        "\n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "# metadata.csv <br />\n",
        "audio1|This is my sentence. <br />\n",
        "audio2|This is maybe my sentence. <br />\n",
        "audio3|This is certainly my sentence. <br />\n",
        "audio4|Let this be your sentence. <br />\n",
        "...\n",
        "</span>\n",
        "\n",
        "In the end, we should have the following **folder structure**:\n",
        "\n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "/MyTTSDataset <br />\n",
        "&emsp;| <br />\n",
        "&emsp;| -> metadata.csv<br />\n",
        "&emsp;| -> /wavs<br />\n",
        "&emsp;&emsp;| -> audio1.wav<br />\n",
        "&emsp;&emsp;| -> audio2.wav<br />\n",
        "&emsp;&emsp;| ...<br />\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69501a10-3b53-4e75-ae66-90221d6f2271",
      "metadata": {
        "id": "69501a10-3b53-4e75-ae66-90221d6f2271"
      },
      "source": [
        "üê∏TTS already provides tooling for the _LJSpeech_. if you use the same format, you can start training your models right away. <br />\n",
        "\n",
        "After you collect and format your dataset, you need to check two things. Whether you need a **_formatter_** and a **_text_cleaner_**. <br /> The **_formatter_** loads the text file (created above) as a list and the **_text_cleaner_** performs a sequence of text normalization operations that converts the raw text into the spoken representation (e.g. converting numbers to text, acronyms, and symbols to the spoken format).\n",
        "\n",
        "If you use a different dataset format then the LJSpeech or the other public datasets that üê∏TTS supports, then you need to write your own **_formatter_** and  **_text_cleaner_**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f226c8-4e55-48fa-937b-8415d539b17c",
      "metadata": {
        "id": "e7f226c8-4e55-48fa-937b-8415d539b17c"
      },
      "source": [
        "## ‚è≥Ô∏è Loading your dataset\n",
        "Load one of the dataset supported by üê∏TTS.\n",
        "\n",
        "We will start by defining dataset config and setting LJSpeech as our target dataset and define its path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66",
      "metadata": {
        "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "\n",
        "output_path = \"tts_train_dir\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6b7019-3685-4b48-8917-c152e288d7e3",
      "metadata": {
        "id": "ae6b7019-3685-4b48-8917-c152e288d7e3"
      },
      "outputs": [],
      "source": [
        "# Download and extract LJSpeech dataset.\n",
        "\n",
        "!wget -O $output_path/LJSpeech-1.1.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "!tar -xf $output_path/LJSpeech-1.1.tar.bz2 -C $output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5",
      "metadata": {
        "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5"
      },
      "outputs": [],
      "source": [
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"LJSpeech-1.1/\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae82fd75",
      "metadata": {
        "id": "ae82fd75"
      },
      "source": [
        "## ‚úÖ Train a new model\n",
        "\n",
        "Let's kick off a training run üöÄüöÄüöÄ.\n",
        "\n",
        "Deciding on the model architecture you'd want to use is based on your needs and available resources. Each model architecture has it's pros and cons that define the run-time efficiency and the voice quality.\n",
        "We have many recipes under `TTS/recipes/` that provide a good starting point. For this tutorial, we will be using `GlowTTS`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f",
      "metadata": {
        "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f"
      },
      "source": [
        "We will begin by initializing the model training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84",
      "metadata": {
        "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84"
      },
      "outputs": [],
      "source": [
        "# GlowTTSConfig: all model related values for training, validating and testing.\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=100,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    save_step=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93ed377-80b7-447b-bd92-106bffa777ee",
      "metadata": {
        "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
      },
      "source": [
        "Next we will initialize the audio processor which is used for feature extraction and audio I/O."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
      "metadata": {
        "id": "b1b12f61-f851-4565-84dd-7640947e04ab"
      },
      "outputs": [],
      "source": [
        "from TTS.utils.audio import AudioProcessor\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "# Modify sample rate if for a custom audio dataset:\n",
        "# ap.sample_rate = 22050\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d461683-b05e-403f-815f-8007bda08c38",
      "metadata": {
        "id": "1d461683-b05e-403f-815f-8007bda08c38"
      },
      "source": [
        "Next we will initialize the tokenizer which is used to convert text to sequences of token IDs.  If characters are not defined in the config, default characters are passed to the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014879b7-f18d-44c0-b24a-e10f8002113a",
      "metadata": {
        "id": "014879b7-f18d-44c0-b24a-e10f8002113a"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978",
      "metadata": {
        "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978"
      },
      "source": [
        "Next we will load data samples. Each sample is a list of ```[text, audio_file_path, speaker_name]```. You can define your custom sample loader returning the list of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
      "metadata": {
        "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.datasets import load_tts_samples\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19",
      "metadata": {
        "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
      },
      "source": [
        "Now we're ready to initialize the model.\n",
        "\n",
        "Models take a config object and a speaker manager as input. Config defines the details of the model like the number of layers, the size of the embedding, etc. Speaker manager is used by multi-speaker models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4",
      "metadata": {
        "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2832c56-889d-49a6-95b6-eb231892ecc6",
      "metadata": {
        "id": "e2832c56-889d-49a6-95b6-eb231892ecc6"
      },
      "source": [
        "Trainer provides a generic API to train all the üê∏TTS models with all its perks like mixed-precision training, distributed training, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
      "metadata": {
        "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe"
      },
      "outputs": [],
      "source": [
        "from trainer import Trainer, TrainerArgs\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b320831-dd83-429b-bb6a-473f9d49d321",
      "metadata": {
        "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
      },
      "source": [
        "### AND... 3,2,1... START TRAINING üöÄüöÄüöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
      "metadata": {
        "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f"
      },
      "outputs": [],
      "source": [
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98",
      "metadata": {
        "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98"
      },
      "source": [
        "#### üöÄ Run the Tensorboard. üöÄ\n",
        "On the notebook and Tensorboard, you can monitor the progress of your model. Also Tensorboard provides certain figures and sample outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec",
      "metadata": {
        "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "!tensorboard --logdir=tts_train_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6dc959",
      "metadata": {
        "id": "9f6dc959"
      },
      "source": [
        "## ‚úÖ Test the model\n",
        "\n",
        "We made it! üôå\n",
        "\n",
        "Let's kick off the testing run, which displays performance metrics.\n",
        "\n",
        "We're committing the cardinal sin of ML üòà (aka - testing on our training data) so you don't want to deploy this model into production. In this notebook we're focusing on the workflow itself, so it's forgivable üòá\n",
        "\n",
        "You can see from the test output that our tiny model has overfit to the data, and basically memorized this one sentence.\n",
        "\n",
        "When you start training your own models, make sure your testing data doesn't include your training data üòÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0",
      "metadata": {
        "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0"
      },
      "source": [
        "Let's get the latest saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961",
      "metadata": {
        "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "output_path = \"tts_train_dir\"\n",
        "ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n",
        "configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd42bc7a",
      "metadata": {
        "id": "dd42bc7a"
      },
      "outputs": [],
      "source": [
        " !tts --text \"Text for TTS\" \\\n",
        "      --model_path $test_ckpt \\\n",
        "      --config_path $test_config \\\n",
        "      --out_path out.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670",
      "metadata": {
        "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670"
      },
      "source": [
        "## üì£ Listen to the synthesized wave üì£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff",
      "metadata": {
        "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff",
        "outputId": "88fade4f-597e-406f-9862-8579586684ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cssselect\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: cssselect\n",
            "Successfully installed cssselect-1.3.0\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cssselect\n",
        "!pip install lxml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13914401-cad1-494a-b701-474e52829138",
      "metadata": {
        "id": "13914401-cad1-494a-b701-474e52829138"
      },
      "source": [
        "## üéâ Congratulations! üéâ You now have trained your first TTS model!\n",
        "Follow up with the next tutorials to learn more advanced material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7",
      "metadata": {
        "collapsed": true,
        "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7",
        "outputId": "e167f7d1-0034-4b41-89b9-387afa6b886d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Fetching page 7...\n",
            "Fetching page 8...\n",
            "Fetching page 9...\n",
            "Fetching page 10...\n",
            "Finished page 3. Found 20 titles.\n",
            "Fetching page 11...\n",
            "Finished page 9. Found 20 titles.\n",
            "Finished page 6. Found 20 titles.\n",
            "Finished page 10. Found 20 titles.\n",
            "Finished page 8. Found 20 titles.\n",
            "Finished page 4. Found 20 titles.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bs4/element.py:1300: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  u = str.__new__(cls, value)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished page 7. Found 20 titles.\n",
            "Finished page 2. Found 20 titles.\n",
            "Fetching page 12...\n",
            "Fetching page 13...\n",
            "Fetching page 14...\n",
            "Fetching page 15...\n",
            "Fetching page 16...\n",
            "Fetching page 17...\n",
            "Fetching page 18...\n",
            "Finished page 1. Found 20 titles.\n",
            "Finished page 5. Found 20 titles.\n",
            "Fetching page 19...\n",
            "Fetching page 20...\n",
            "Finished page 16. Found 20 titles.\n",
            "Fetching page 21...\n",
            "Finished page 14. Found 20 titles.\n",
            "Finished page 15. Found 20 titles.\n",
            "Finished page 12. Found 20 titles.\n",
            "Finished page 13. Found 20 titles.\n",
            "Finished page 11. Found 20 titles.\n",
            "Finished page 18. Found 20 titles.\n",
            "Finished page 17. Found 20 titles.\n",
            "Fetching page 22...\n",
            "Fetching page 23...\n",
            "Fetching page 24...\n",
            "Fetching page 25...\n",
            "Fetching page 26...\n",
            "Fetching page 27...\n",
            "Fetching page 28...\n",
            "Finished page 20. Found 20 titles.\n",
            "Finished page 19. Found 20 titles.\n",
            "Fetching page 29...\n",
            "Fetching page 30...\n",
            "Finished page 22. Found 20 titles.\n",
            "Fetching page 31...\n",
            "Finished page 25. Found 20 titles.\n",
            "Finished page 21. Found 20 titles.\n",
            "Finished page 23. Found 20 titles.\n",
            "Finished page 26. Found 20 titles.\n",
            "Finished page 28. Found 20 titles.\n",
            "Finished page 27. Found 20 titles.\n",
            "Finished page 24. Found 20 titles.\n",
            "Fetching page 32...\n",
            "Fetching page 33...\n",
            "Fetching page 34...\n",
            "Fetching page 35...\n",
            "Fetching page 36...\n",
            "Fetching page 37...\n",
            "Fetching page 38...\n",
            "Finished page 29. Found 20 titles.\n",
            "Finished page 30. Found 20 titles.\n",
            "Fetching page 39...\n",
            "Fetching page 40...\n",
            "Finished page 31. Found 20 titles.\n",
            "Fetching page 41...\n",
            "Finished page 35. Found 20 titles.\n",
            "Finished page 36. Found 20 titles.\n",
            "Finished page 38. Found 20 titles.\n",
            "Finished page 37. Found 20 titles.\n",
            "Fetching page 42...\n",
            "Fetching page 43...\n",
            "Fetching page 44...\n",
            "Fetching page 45...\n",
            "Finished page 34. Found 20 titles.\n",
            "Finished page 33. Found 20 titles.\n",
            "Finished page 32. Found 20 titles.\n",
            "Finished page 40. Found 20 titles.\n",
            "Finished page 39. Found 20 titles.\n",
            "Fetching page 46...\n",
            "Fetching page 47...\n",
            "Fetching page 48...\n",
            "Fetching page 49...\n",
            "Fetching page 50...\n",
            "Finished page 41. Found 20 titles.\n",
            "Fetching page 51...\n",
            "Finished page 44. Found 20 titles.\n",
            "Finished page 43. Found 20 titles.\n",
            "Finished page 45. Found 20 titles.\n",
            "Finished page 42. Found 20 titles.\n",
            "Fetching page 52...\n",
            "Fetching page 53...\n",
            "Fetching page 54...\n",
            "Fetching page 55...\n",
            "Finished page 47. Found 20 titles.\n",
            "Finished page 48. Found 20 titles.\n",
            "Finished page 46. Found 20 titles.\n",
            "Finished page 50. Found 20 titles.\n",
            "Finished page 49. Found 20 titles.\n",
            "Fetching page 56...\n",
            "Fetching page 57...\n",
            "Fetching page 58...\n",
            "Fetching page 59...\n",
            "Fetching page 60...\n",
            "Finished page 53. Found 20 titles.\n",
            "Fetching page 61...\n",
            "Finished page 54. Found 20 titles.\n",
            "Finished page 51. Found 20 titles.\n",
            "Finished page 52. Found 20 titles.\n",
            "Finished page 55. Found 20 titles.\n",
            "Fetching page 62...\n",
            "Fetching page 63...\n",
            "Fetching page 64...\n",
            "Fetching page 65...\n",
            "Finished page 60. Found 20 titles.\n",
            "Finished page 56. Found 20 titles.\n",
            "Finished page 57. Found 20 titles.\n",
            "Finished page 59. Found 20 titles.\n",
            "Finished page 58. Found 20 titles.\n",
            "Fetching page 66...\n",
            "Fetching page 67...\n",
            "Fetching page 68...\n",
            "Fetching page 69...\n",
            "Fetching page 70...\n",
            "Finished page 64. Found 20 titles.\n",
            "Fetching page 71...\n",
            "Finished page 65. Found 20 titles.\n",
            "Finished page 63. Found 20 titles.\n",
            "Finished page 61. Found 20 titles.\n",
            "Finished page 62. Found 20 titles.\n",
            "Fetching page 72...\n",
            "Fetching page 73...\n",
            "Fetching page 74...\n",
            "Fetching page 75...\n",
            "Finished page 67. Found 20 titles.\n",
            "Finished page 70. Found 20 titles.\n",
            "Finished page 66. Found 20 titles.\n",
            "Finished page 69. Found 20 titles.\n",
            "Finished page 68. Found 20 titles.\n",
            "Fetching page 76...\n",
            "Fetching page 77...\n",
            "Fetching page 78...\n",
            "Fetching page 79...\n",
            "Fetching page 80...\n",
            "Finished page 74. Found 20 titles.\n",
            "Fetching page 81...\n",
            "Finished page 72. Found 20 titles.\n",
            "Finished page 71. Found 20 titles.\n",
            "Finished page 75. Found 20 titles.\n",
            "Fetching page 82...\n",
            "Fetching page 83...\n",
            "Fetching page 84...\n",
            "Finished page 73. Found 20 titles.\n",
            "Fetching page 85...\n",
            "Finished page 78. Found 20 titles.\n",
            "Finished page 76. Found 20 titles.\n",
            "Finished page 80. Found 20 titles.\n",
            "Finished page 77. Found 20 titles.\n",
            "Finished page 79. Found 20 titles.\n",
            "Fetching page 86...\n",
            "Fetching page 87...\n",
            "Fetching page 88...\n",
            "Fetching page 89...\n",
            "Fetching page 90...\n",
            "Finished page 85. Found 20 titles.\n",
            "Fetching page 91...\n",
            "Finished page 83. Found 20 titles.\n",
            "Finished page 82. Found 20 titles.\n",
            "Finished page 81. Found 20 titles.\n",
            "Fetching page 92...\n",
            "Fetching page 93...\n",
            "Fetching page 94...\n",
            "Finished page 84. Found 20 titles.\n",
            "Finished page 90. Found 20 titles.\n",
            "Finished page 86. Found 20 titles.\n",
            "Finished page 89. Found 20 titles.\n",
            "Finished page 88. Found 20 titles.\n",
            "Finished page 87. Found 20 titles.\n",
            "Fetching page 95...\n",
            "Fetching page 96...\n",
            "Fetching page 97...\n",
            "Fetching page 98...\n",
            "Fetching page 99...\n",
            "Fetching page 100...\n",
            "Finished page 94. Found 20 titles.\n",
            "Fetching page 101...\n",
            "Finished page 92. Found 20 titles.\n",
            "Finished page 91. Found 20 titles.\n",
            "Finished page 93. Found 20 titles.\n",
            "Fetching page 102...\n",
            "Fetching page 103...\n",
            "Fetching page 104...\n",
            "Finished page 95. Found 20 titles.\n",
            "Finished page 97. Found 20 titles.\n",
            "Finished page 99. Found 20 titles.\n",
            "Finished page 100. Found 20 titles.\n",
            "Finished page 96. Found 20 titles.\n",
            "Finished page 98. Found 20 titles.\n",
            "Fetching page 105...\n",
            "Fetching page 106...\n",
            "Fetching page 107...\n",
            "Fetching page 108...\n",
            "Fetching page 109...\n",
            "Fetching page 110...\n",
            "Finished page 103. Found 20 titles.\n",
            "Fetching page 111...\n",
            "Finished page 101. Found 20 titles.\n",
            "Finished page 104. Found 20 titles.\n",
            "Finished page 102. Found 20 titles.\n",
            "Fetching page 112...\n",
            "Fetching page 113...\n",
            "Fetching page 114...\n",
            "Finished page 110. Found 20 titles.\n",
            "Finished page 105. Found 20 titles.\n",
            "Fetching page 115...\n",
            "Fetching page 116...\n",
            "Finished page 107. Found 20 titles.\n",
            "Finished page 109. Found 20 titles.\n",
            "Finished page 106. Found 20 titles.\n",
            "Finished page 108. Found 20 titles.\n",
            "Fetching page 117...\n",
            "Fetching page 118...\n",
            "Fetching page 119...\n",
            "Fetching page 120...\n",
            "Finished page 112. Found 20 titles.\n",
            "Fetching page 121...\n",
            "Finished page 111. Found 20 titles.\n",
            "Finished page 115. Found 20 titles.\n",
            "Finished page 113. Found 20 titles.\n",
            "Finished page 114. Found 20 titles.\n",
            "Finished page 116. Found 20 titles.\n",
            "Fetching page 122...\n",
            "Fetching page 123...\n",
            "Fetching page 124...\n",
            "Fetching page 125...\n",
            "Fetching page 126...\n",
            "Finished page 117. Found 20 titles.\n",
            "Finished page 120. Found 20 titles.\n",
            "Finished page 118. Found 20 titles.\n",
            "Finished page 119. Found 20 titles.\n",
            "Fetching page 127...\n",
            "Fetching page 128...\n",
            "Fetching page 129...\n",
            "Fetching page 130...\n",
            "Finished page 122. Found 20 titles.\n",
            "Fetching page 131...\n",
            "Finished page 125. Found 20 titles.\n",
            "Finished page 121. Found 20 titles.\n",
            "Finished page 126. Found 20 titles.\n",
            "Finished page 123. Found 20 titles.\n",
            "Finished page 124. Found 20 titles.\n",
            "Fetching page 132...\n",
            "Fetching page 133...\n",
            "Fetching page 134...\n",
            "Fetching page 135...\n",
            "Fetching page 136...\n",
            "Finished page 128. Found 20 titles.\n",
            "Finished page 130. Found 20 titles.\n",
            "Finished page 127. Found 20 titles.\n",
            "Finished page 129. Found 20 titles.\n",
            "Fetching page 137...\n",
            "Fetching page 138...\n",
            "Fetching page 139...\n",
            "Fetching page 140...\n",
            "Finished page 136. Found 20 titles.\n",
            "Fetching page 141...\n",
            "Finished page 132. Found 20 titles.\n",
            "Finished page 133. Found 20 titles.\n",
            "Finished page 131. Found 20 titles.\n",
            "Finished page 134. Found 20 titles.\n",
            "Finished page 135. Found 20 titles.\n",
            "Fetching page 142...\n",
            "Fetching page 143...\n",
            "Fetching page 144...\n",
            "Fetching page 145...\n",
            "Fetching page 146...\n",
            "Finished page 138. Found 20 titles.\n",
            "Finished page 140. Found 20 titles.\n",
            "Finished page 139. Found 20 titles.\n",
            "Finished page 137. Found 20 titles.\n",
            "Fetching page 147...\n",
            "Fetching page 148...\n",
            "Fetching page 149...\n",
            "Fetching page 150...\n",
            "Finished page 145. Found 20 titles.\n",
            "Fetching page 151...\n",
            "Finished page 141. Found 20 titles.\n",
            "Finished page 142. Found 20 titles.\n",
            "Finished page 143. Found 20 titles.\n",
            "Fetching page 152...\n",
            "Fetching page 153...\n",
            "Fetching page 154...\n",
            "Finished page 146. Found 20 titles.\n",
            "Finished page 144. Found 20 titles.\n",
            "Finished page 149. Found 20 titles.\n",
            "Finished page 150. Found 20 titles.\n",
            "Finished page 148. Found 20 titles.\n",
            "Finished page 147. Found 20 titles.\n",
            "Fetching page 155...\n",
            "Fetching page 156...\n",
            "Finished page 154. Found 20 titles.\n",
            "Finished page 153. Found 20 titles.\n",
            "Finished page 151. Found 20 titles.\n",
            "Finished page 152. Found 20 titles.\n",
            "Finished page 156. Found 19 titles.\n",
            "Finished page 155. Found 20 titles.\n",
            "\n",
            "Finished scraping 3119 titles in 17.43 seconds.\n",
            "All movie titles saved to 'kurdfilm_movie_titles_async.txt'\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "import lxml.html\n",
        "import json\n",
        "import logging\n",
        "import urllib.parse\n",
        "import nest_asyncio\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "BASE_URL = \"https://kurdfilm.krd\"\n",
        "BASE_PAGE_URL = \"https://kurdfilm.krd/va/m?_token=2wrbDq29KA94sDjR15hRDTOXyf9DCeJchMCHi8fE&type=all&genre=genre&releasedate=year\"\n",
        "\n",
        "START_PAGE = 1\n",
        "END_PAGE = 156\n",
        "\n",
        "async def fetch_page(client: httpx.AsyncClient, page_num: int):\n",
        "    \"\"\"Fetches a single page and returns the HTML content.\"\"\"\n",
        "    url = f\"{BASE_PAGE_URL}&page={page_num}\"\n",
        "    logging.info(f\"Fetching page {page_num}: {url}\")\n",
        "    try:\n",
        "        response = await client.get(url, timeout=30.0)\n",
        "        response.raise_for_status()\n",
        "        logging.info(f\"Successfully fetched page {page_num}\")\n",
        "        return response.text\n",
        "    except httpx.RequestError as e:\n",
        "        logging.error(f\"An HTTP error occurred while fetching page {page_num}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred while fetching page {page_num}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_movie_data(html_content: str):\n",
        "    \"\"\"Parses HTML content and extracts movie details.\"\"\"\n",
        "    movies = []\n",
        "    if not html_content:\n",
        "        return movies\n",
        "\n",
        "    try:\n",
        "        tree = lxml.html.fromstring(html_content)\n",
        "        movie_elements = tree.xpath('//div[contains(@class, \"row\")]/div[contains(@class, \"col-6\")]')\n",
        "\n",
        "        for movie_el in movie_elements:\n",
        "            try:\n",
        "                link_el = movie_el.xpath('.//a[contains(@class, \"stretched-link\")]')[0]\n",
        "                movie_link = urllib.parse.urljoin(BASE_URL, link_el.get('href'))\n",
        "\n",
        "                img_el = link_el.xpath('.//img[contains(@class, \"img-fluid\")]')[0]\n",
        "                movie_img_url = urllib.parse.urljoin(BASE_URL, img_el.get('src'))\n",
        "\n",
        "                title_el = movie_el.xpath('.//div[contains(@class, \"product-title\")]/a')[0]\n",
        "                movie_title = title_el.text_content().strip()\n",
        "\n",
        "                genre_el = movie_el.xpath('.//div[contains(@class, \"product-meta\")]/span[@class=\"text-gray-1300\"]')\n",
        "                genre = genre_el[0].text_content().strip() if genre_el else None\n",
        "\n",
        "                rating = None\n",
        "                if genre_el:\n",
        "                    text_content = genre_el[0].text_content().strip()\n",
        "                    if '10/' in text_content:\n",
        "                        rating_part = text_content.split('10/')[1].split('<i')[0].split(' ')[0]\n",
        "                        try:\n",
        "                            rating = float(rating_part)\n",
        "                        except ValueError:\n",
        "                            rating = rating_part\n",
        "\n",
        "                movies.append({\n",
        "                    \"title\": movie_title,\n",
        "                    \"link\": movie_link,\n",
        "                    \"image_url\": movie_img_url,\n",
        "                    \"genre\": genre,\n",
        "                    \"rating\": rating\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error parsing a movie element on the page: {e}\", exc_info=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing HTML content: {e}\", exc_info=True)\n",
        "\n",
        "    return movies\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function to orchestrate fetching and parsing multiple pages.\"\"\"\n",
        "    all_movies = []\n",
        "    pages_to_fetch = list(range(START_PAGE, END_PAGE + 1))\n",
        "\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        tasks = [fetch_page(client, page) for page in pages_to_fetch]\n",
        "        html_contents = await asyncio.gather(*tasks)\n",
        "\n",
        "    for page_num, html_content in zip(pages_to_fetch, html_contents):\n",
        "        if html_content:\n",
        "            logging.info(f\"Parsing data from page {page_num}\")\n",
        "            movies_on_page = parse_movie_data(html_content)\n",
        "            all_movies.extend(movies_on_page)\n",
        "            logging.info(f\"Found {len(movies_on_page)} movies on page {page_num}. Total movies collected: {len(all_movies)}\")\n",
        "        else:\n",
        "            logging.warning(f\"Skipping parsing for page {page_num} due to fetch error.\")\n",
        "\n",
        "    try:\n",
        "        with open(\"kurdfilm_movies.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_movies, f, ensure_ascii=False, indent=4)\n",
        "        logging.info(f\"Successfully scraped {len(all_movies)} movies and saved to kurdfilm_movies.json\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving data to JSON file: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"Starting scraper for pages {START_PAGE} to {END_PAGE}...\")\n",
        "    nest_asyncio.apply()  # Apply nest_asyncio patch\n",
        "    asyncio.run(main())\n",
        "    logging.info(\"Scraping finished.\")"
      ],
      "metadata": {
        "id": "ecn4-P4_mb39"
      },
      "id": "ecn4-P4_mb39",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sb9eJMyf3lId"
      },
      "id": "sb9eJMyf3lId"
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "import asyncio\n",
        "import json\n",
        "from lxml import html\n",
        "\n",
        "BASE_URL = \"https://kurdfilm.krd/va/t?type=all&page={}\"  # Pages 1-20\n",
        "SITE_ROOT = \"https://kurdfilm.krd\"\n",
        "\n",
        "async def fetch_page(client, page_num):\n",
        "    url = BASE_URL.format(page_num)\n",
        "    try:\n",
        "        response = await client.get(url, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except httpx.HTTPError as e:\n",
        "        print(f\"Failed to fetch page {page_num}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_tvshows(html_content):\n",
        "    tree = html.fromstring(html_content)\n",
        "    tvshows = []\n",
        "\n",
        "    for product in tree.xpath('//div[contains(@class, \"product\")]'):\n",
        "        title_el = product.xpath('.//div[contains(@class, \"product-title\")]/a')\n",
        "        img_el = product.xpath('.//div[contains(@class, \"product-image\")]//img')\n",
        "\n",
        "        if not title_el or not img_el:\n",
        "            continue\n",
        "\n",
        "        title = title_el[0].text_content().strip()\n",
        "        link = title_el[0].get(\"href\")\n",
        "        image = img_el[0].get(\"src\")\n",
        "\n",
        "        if not link.startswith(\"http\"):\n",
        "            link = SITE_ROOT + link\n",
        "        if not image.startswith(\"http\"):\n",
        "            image = SITE_ROOT + image\n",
        "\n",
        "        tvshows.append({\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"image\": image\n",
        "        })\n",
        "    return tvshows\n",
        "\n",
        "async def main():\n",
        "    tvshows_data = []\n",
        "    async with httpx.AsyncClient(http2=True) as client:\n",
        "        tasks = [fetch_page(client, i) for i in range(1, 21)]\n",
        "        pages = await asyncio.gather(*tasks)\n",
        "\n",
        "        for content in pages:\n",
        "            if content:\n",
        "                tvshows_data.extend(extract_tvshows(content))\n",
        "\n",
        "    with open(\"tvshows.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(tvshows_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Scraped {len(tvshows_data)} TV shows and saved to 'tvshows.json'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "KKaonpa8xIa5",
        "outputId": "25fd92e7-eaf6-453f-8aa0-394fc70bbc07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KKaonpa8xIa5",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Scraped 519 TV shows and saved to 'tvshows.json'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}